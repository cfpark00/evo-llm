# ES Evolution Experiment Configuration

# Output configuration
output_dir: "data/es_conciseness_v1"
seed: 0

# Task configuration
task:
  name: "conciseness"
  train_data: "data/tasks/conciseness/conciseness_train.json"
  test_data: "data/tasks/conciseness/conciseness_test.json"

# Model configuration
model:
  name: "Qwen/Qwen2.5-7B-Instruct"

# LoRA configuration
lora:
  rank: 8
  alpha: 16
  target_layers: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]  # Layers 0-9
  target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]

# ES hyperparameters
es:
  num_generations: 1000
  population_size: 30
  learning_rate: 0.01
  init_scale: 4      # Multiplier for Gen0 initialization (applied to per-layer base noise)
  perturb_scale: 0.2   # Multiplier for Gen1+ perturbations (applied to per-layer base noise)

# Evaluation configuration
evaluation:
  max_tokens: 100
  temperature: 0.0  # Greedy decoding
