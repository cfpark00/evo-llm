EVOLM - Evolution Strategies for LLM Fine-tuning
Repository Structure
Last Updated: 2025-10-21

================================================================================
ROOT DIRECTORY
================================================================================
/n/holylfs06/LABS/finkbeiner_lab/Users/cfpark00/datadir/evolm/

CONFIGURATION FILES
-------------------
â”œâ”€â”€ CLAUDE.md              Project development guidelines (mandatory)
â”œâ”€â”€ README.md              Basic project information
â”œâ”€â”€ pyproject.toml         UV package configuration & dependencies
â”œâ”€â”€ uv.lock                Locked dependencies for reproducibility
â”œâ”€â”€ .env                   Environment variables (DATA_DIR=./data/)
â”œâ”€â”€ .env.example           Template for environment setup
â”œâ”€â”€ .gitignore             Git ignore rules (data/, .venv/, scratch/)

PYTHON VIRTUAL ENVIRONMENT
--------------------------
â”œâ”€â”€ .venv/                 UV-managed Python virtual environment
                           (Use: source .venv/bin/activate OR uv run)

================================================================================
SOURCE CODE (/src/)
================================================================================
All production Python code lives here.

â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ utils.py           Safe directory initialization (init_directory)
â”‚   â”‚                      - Validates output paths against DATA_DIR
â”‚   â”‚                      - Prevents accidental overwrites outside data/
â”‚   â”‚                      - Used by all orchestration scripts
â”‚   â”‚
â”‚   â”œâ”€â”€ tasks/             Task-specific logic (reward functions, data loading)
â”‚   â”‚   â”œâ”€â”€ __init__.py    Task registry (get_task function)
â”‚   â”‚   â”œâ”€â”€ conciseness.py ConcisenessTask class
â”‚   â”‚   â””â”€â”€ countdown.py   CountdownTask class
â”‚   â”‚
â”‚   â””â”€â”€ scripts/           Orchestration scripts (entry points)
â”‚       â””â”€â”€ es_train.py    Main ES training loop (parallel multi-LoRA)

================================================================================
CONFIGURATION FILES (/configs/)
================================================================================
All experiment configurations in YAML format.
Every config MUST have 'output_dir' field.

â”œâ”€â”€ configs/               [EMPTY] Currently no configs
â”‚                          Expected future structure:
â”‚                          â”œâ”€â”€ experiments/
â”‚                          â”‚   â””â”€â”€ countdown_es.yaml
â”‚                          â”œâ”€â”€ models/
â”‚                          â”‚   â””â”€â”€ qwen_lora.yaml
â”‚                          â””â”€â”€ data/
â”‚                              â””â”€â”€ countdown.yaml

================================================================================
BASH SCRIPTS (/scripts/)
================================================================================
Minimal bash wrappers for running experiments.
Pattern: uv run python src/scripts/<script>.py configs/<config>.yaml "$@"

â”œâ”€â”€ scripts/               [EMPTY] Currently no scripts
â”‚                          Expected future structure:
â”‚                          â”œâ”€â”€ experiments/
â”‚                          â”‚   â””â”€â”€ train_countdown_es.sh
â”‚                          â””â”€â”€ evaluation/
â”‚                              â””â”€â”€ eval_checkpoint.sh

================================================================================
DATA OUTPUTS (/data/)
================================================================================
All experiment outputs (gitignored).
Auto-created from configs' output_dir field.

â”œâ”€â”€ data/                  [EMPTY] Experiment outputs go here
â”‚                          Standard structure per experiment:
â”‚                          â””â”€â”€ <experiment_name>/
â”‚                              â”œâ”€â”€ figures/       Plots & visualizations
â”‚                              â”œâ”€â”€ results/       JSON/CSV results
â”‚                              â”œâ”€â”€ logs/          Training logs
â”‚                              â””â”€â”€ config.yaml    Copy of config used

================================================================================
DOCUMENTATION (/docs/)
================================================================================
Research context, development logs, and technical documentation.

â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ start.txt                 Quick start guide (points to key docs)
â”‚   â”œâ”€â”€ closing_tasks.md          End-of-day checklist
â”‚   â”œâ”€â”€ structure.txt             This file - repo structure overview
â”‚   â”‚
â”‚   â”œâ”€â”€ CLAUDE.md -> ../CLAUDE.md Symlink to root CLAUDE.md
â”‚   â”‚
â”‚   â”œâ”€â”€ repo_usage.md             Comprehensive development guide (213 lines)
â”‚   â”‚                             - Fail-fast philosophy
â”‚   â”‚                             - Implementation vs orchestration
â”‚   â”‚                             - Config validation patterns
â”‚   â”‚                             - Standard script templates
â”‚   â”‚
â”‚   â”œâ”€â”€ research_context.md       Main research document (237 lines)
â”‚   â”‚                             - Project goals & methodology
â”‚   â”‚                             - ES algorithm details
â”‚   â”‚                             - Current progress & milestones
â”‚   â”‚                             - Open questions & hypotheses
â”‚   â”‚
â”‚   â””â”€â”€ logs/                     Daily development logs
â”‚       â””â”€â”€ 2025-10-21/
â”‚           â”œâ”€â”€ 1214_infrastructure_setup_and_testing.md
â”‚           â”œâ”€â”€ 1452_sglang_lora_fix.md
â”‚           â””â”€â”€ 1748_es_conciseness_task_complete_setup.md

================================================================================
EXTERNAL RESOURCES (/resources/)
================================================================================
External code, papers, datasets (gitignored).

â”œâ”€â”€ resources/
â”‚   â””â”€â”€ es-fine-tuning-paper/     Git submodule - original paper's code
â”‚       â”œâ”€â”€ README.md
â”‚       â”œâ”€â”€ LICENSE.txt
â”‚       â”œâ”€â”€ requirement.txt
â”‚       â”œâ”€â”€ es_fine-tuning_conciseness.py
â”‚       â”œâ”€â”€ es_fine-tuning_conciseness_iid.py
â”‚       â”‚
â”‚       â””â”€â”€ countdown/            Countdown task implementation
â”‚           â”œâ”€â”€ countdown_task.py Reward function (imported by tests)
â”‚           â”œâ”€â”€ es_fine-tuning_countdown.py
â”‚           â”œâ”€â”€ es_fine-tuning_countdown_iid.py
â”‚           â””â”€â”€ data/
â”‚               â””â”€â”€ countdown.json    Dataset (5+ examples)

================================================================================
SCRATCH SPACE (/scratch/)
================================================================================
Temporary testing & exploration code (gitignored except .gitkeep).
NOT production code - for experimentation only.

â”œâ”€â”€ scratch/
â”‚   â”œâ”€â”€ .gitkeep
â”‚   â”‚
â”‚   â”œâ”€â”€ countdown_test/           Countdown task testing
â”‚   â”‚   â”œâ”€â”€ test_countdown.py     (183 lines) Base model inference test
â”‚   â”‚   â”œâ”€â”€ run_test.sh           Executable bash script
â”‚   â”‚   â””â”€â”€ outputs/              Test results directory
â”‚   â”‚
â”‚   â””â”€â”€ sglang_lora_test/         SGLang + LoRA integration testing
â”‚       â”œâ”€â”€ init_loras.py         (165 lines) Create 5 LoRA adapters
â”‚       â”œâ”€â”€ es_test_run.py        (389 lines) ES loop skeleton
â”‚       â”œâ”€â”€ run_init.sh           Run LoRA initialization
â”‚       â”œâ”€â”€ run_es_test.sh        Run ES test loop
â”‚       â”œâ”€â”€ start_sglang_server.sh    Start SGLang with multi-LoRA
â”‚       â”‚
â”‚       â”œâ”€â”€ lora_adapters/        Generated LoRA adapters
â”‚       â”‚   â”œâ”€â”€ lora_0/           Adapter 0 (rank=1, alpha=2)
â”‚       â”‚   â”‚   â”œâ”€â”€ adapter_config.json
â”‚       â”‚   â”‚   â””â”€â”€ adapter_model.safetensors
â”‚       â”‚   â”œâ”€â”€ lora_1/ ... lora_4/   Adapters 1-4 (same config)
â”‚       â”‚   â”œâ”€â”€ tokenizer/        Shared tokenizer
â”‚       â”‚   â””â”€â”€ metadata.json     Configuration tracking
â”‚       â”‚
â”‚       â””â”€â”€ es_test_output/       [Created at runtime]
â”‚                                  Evolution history & checkpoints

================================================================================
KEY DEPENDENCIES (from pyproject.toml)
================================================================================

CORE ML FRAMEWORKS
------------------
- torch                   PyTorch deep learning framework
- transformers            HuggingFace transformers (LLM inference)
- peft                    Parameter-Efficient Fine-Tuning (LoRA)
- accelerate              Distributed training utilities

SERVING & API
-------------
- fastapi                 Web service framework
- requests                HTTP client (for SGLang API)
- sglang                  [External] Multi-LoRA serving (not in pyproject)

EXPERIMENT TRACKING
-------------------
- wandb                   Weights & Biases experiment tracking

DATA SCIENCE
------------
- numpy                   Numerical computing
- scipy                   Scientific computing
- pandas                  Data manipulation
- scikit-learn            Machine learning utilities

PIPELINE ORCHESTRATION
----------------------
- dspy-ai                 Pipeline orchestration framework

DEVELOPMENT TOOLS
-----------------
- ipykernel               Jupyter kernel
- pytest-cov              Test coverage
- black                   Code formatting
- ruff                    Linting

================================================================================
CURRENT PROJECT STATUS
================================================================================

COMPLETED COMPONENTS
--------------------
âœ… Project structure (following CLAUDE.md)
âœ… Environment setup (uv, dependencies, .env)
âœ… Documentation (research context, development guidelines)
âœ… Countdown task integration (reward function tested)
âœ… LoRA adapter creation (5 adapters, rank=1, alpha=2, layers 0-10)
âœ… Base model inference test (Qwen2.5-7B-Instruct)
âœ… ES loop skeleton (evaluation, perturbation, aggregation logic)

IN PROGRESS
-----------
ğŸ”„ SGLang + LoRA integration (API tested, dynamic loading blocked)
ğŸ”„ End-to-end ES pipeline (skeleton done, needs integration)

NOT YET STARTED
---------------
âŒ Production ES framework in src/
âŒ Experiment configs in configs/
âŒ Orchestration scripts in src/scripts/
âŒ Bash runners in scripts/
âŒ Baseline experiments
âŒ Experiment tracking (W&B integration)
âŒ Unit tests

CRITICAL BLOCKER
----------------
âš ï¸  SGLang dynamic LoRA loading API not documented/working
    - Current: Must preload all LoRAs at server startup
    - Needed: Ability to load new LoRAs during evolution
    - Workaround: Restart server each generation (slow)

================================================================================
RESEARCH CONFIGURATION
================================================================================

MODEL
-----
Base: Qwen/Qwen2.5-7B-Instruct
LoRA config:
  - Rank: 1
  - Alpha: 2
  - Target layers: 0-10 (11 layers)
  - Target modules: q/k/v/o_proj (attn) + gate/up/down_proj (MLP)
  - Total: 77 target modules
  - Trainable params: ~0.01% of full model

ES ALGORITHM (Simplified NES)
-----------------------------
Population size: 30 (paper) / 5 (current testing)
Noise scale (Ïƒ): 0.001
Learning rate (Î±): 5Ã—10â»â´
Generations: 5 (testing) / more (production)
Evaluation: Greedy decoding (deterministic)
Advantage: Z-score normalization (reward - mean) / std

TASK
----
Primary: Countdown (mathematical reasoning)
Dataset: resources/es-fine-tuning-paper/countdown/data/countdown.json
Reward: format_reward (0.5) + answer_reward (0.5)
Evaluation examples: 5 (testing) / more (production)

================================================================================
KEY PATHS (Absolute)
================================================================================

Project Root:
/n/holylfs06/LABS/finkbeiner_lab/Users/cfpark00/datadir/evolm

Additional Working Directory:
/n/home12/cfpark00/datadir/evolm

Key Files:
- Countdown data: resources/es-fine-tuning-paper/countdown/data/countdown.json
- Countdown reward: resources/es-fine-tuning-paper/countdown/countdown_task.py
- Test LoRAs: scratch/sglang_lora_test/lora_adapters/
- Utils: src/utils.py

================================================================================
DEVELOPMENT WORKFLOW
================================================================================

1. CREATE EXPERIMENT CONFIG
   â””â”€ configs/experiments/<experiment_name>.yaml
      Must include: output_dir field

2. IMPLEMENT LOGIC
   â”œâ”€ Implementation (HOW): src/<module>.py
   â””â”€ Orchestration (WHAT/WHEN): src/scripts/<script>.py

3. CREATE BASH RUNNER
   â””â”€ scripts/<category>/<script>.sh
      Pattern: uv run python src/scripts/<script>.py configs/<config>.yaml "$@"

4. RUN EXPERIMENT
   â””â”€ bash scripts/<category>/<script>.sh [--overwrite] [--debug]

5. ANALYZE RESULTS
   â””â”€ data/<experiment_name>/
      â”œâ”€ figures/
      â”œâ”€ results/
      â””â”€ logs/

6. END OF DAY
   â”œâ”€ Write log: docs/logs/<YYYY-MM-DD>/<HHMM_topic>.md
   â”œâ”€ Update docs/structure.txt (if structure changed)
   â””â”€ Update docs/research_context.md (current status)

================================================================================
CODING CONVENTIONS
================================================================================

FAIL FAST PHILOSOPHY
--------------------
âœ… Crash immediately on missing configs
âœ… Validate all required fields upfront
âœ… Exit on critical errors (don't silently continue)
âŒ NO silent fallbacks or default values for critical params
âŒ NO hidden implicit behavior

IMPORTS
-------
from src.module import function         # âœ… Good
from src.utils import init_directory    # âœ… Good
from module import function             # âŒ Bad (relative)

CONFIG VALIDATION
-----------------
# Always check output_dir FIRST
if 'output_dir' not in config:
    raise ValueError("FATAL: 'output_dir' required in config")

# Validate other required fields
for field in required_fields:
    if field not in config:
        raise ValueError(f"FATAL: '{field}' required")

OUTPUT MANAGEMENT
-----------------
from src.utils import init_directory

output_dir = init_directory(config['output_dir'], overwrite=args.overwrite)
(output_dir / 'figures').mkdir(parents=True, exist_ok=True)
(output_dir / 'results').mkdir(parents=True, exist_ok=True)
(output_dir / 'logs').mkdir(parents=True, exist_ok=True)

# Copy config to output_dir for reproducibility
shutil.copy(config_path, output_dir / 'config.yaml')

================================================================================
GIT CONVENTIONS
================================================================================

GITIGNORED
----------
- /data/              All experiment outputs
- /scratch/           All testing code (except .gitkeep)
- /resources/         External resources
- /.venv/             Virtual environment
- /.env               Environment variables (use .env.example)

TRACKED
-------
- /src/               All production code
- /configs/           All configuration files
- /scripts/           All bash runners
- /docs/              All documentation
- uv.lock             Locked dependencies
- CLAUDE.md           Development guidelines

COMMIT MESSAGES
---------------
- Descriptive and concise
- Focus on WHY, not WHAT (code shows what)
- Use imperative mood ("Add feature" not "Added feature")

================================================================================
NOTES
================================================================================

1. This is a RESEARCH repository - prioritize clarity and reproducibility
   over performance optimization.

2. All code in /scratch/ is TEMPORARY - move to /src/ when production-ready.

3. NEVER create file variations like _v2, _final, _updated. Use git.

4. When in doubt, FAIL LOUDLY. Silent failures waste compute time.

5. Configs are the source of truth - everything else is code.

6. Documentation is not optional - it's part of research.

================================================================================
END OF STRUCTURE DOCUMENTATION
================================================================================
2025-10-21: Added scratch/sglang_lora_only_test/ for isolated LoRA testing. Fixed LoRA config to use layers_to_transform instead of explicit paths.
