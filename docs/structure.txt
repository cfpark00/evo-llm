EVOLM - Evolution Strategies for LLM Fine-tuning
Repository Structure
Last Updated: 2025-10-21

================================================================================
ROOT DIRECTORY
================================================================================
/n/holylfs06/LABS/finkbeiner_lab/Users/cfpark00/datadir/evolm/

CONFIGURATION FILES
-------------------
â”œâ”€â”€ CLAUDE.md              Project development guidelines (mandatory)
â”œâ”€â”€ README.md              Basic project information
â”œâ”€â”€ pyproject.toml         UV package configuration & dependencies
â”œâ”€â”€ uv.lock                Locked dependencies for reproducibility
â”œâ”€â”€ .env                   Environment variables (DATA_DIR=./data/)
â”œâ”€â”€ .env.example           Template for environment setup
â”œâ”€â”€ .gitignore             Git ignore rules (data/, .venv/, scratch/)

PYTHON VIRTUAL ENVIRONMENT
--------------------------
â”œâ”€â”€ .venv/                 UV-managed Python virtual environment
                           (Use: source .venv/bin/activate OR uv run)

================================================================================
SOURCE CODE (/src/)
================================================================================
All production Python code lives here.

â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ utils.py           Safe directory initialization (init_directory)
â”‚   â”‚                      - Validates output paths against DATA_DIR
â”‚   â”‚                      - Prevents accidental overwrites outside data/
â”‚   â”‚                      - Used by all orchestration scripts
â”‚   â”‚
â”‚   â”œâ”€â”€ tasks/             Task-specific logic (reward functions, data loading)
â”‚   â”‚   â”œâ”€â”€ __init__.py    Task registry (get_task function)
â”‚   â”‚   â”œâ”€â”€ conciseness.py ConcisenessTask class
â”‚   â”‚   â””â”€â”€ countdown.py   CountdownTask class
â”‚   â”‚
â”‚   â””â”€â”€ scripts/           Orchestration scripts (entry points)
â”‚       â””â”€â”€ es_train.py    Main ES training loop (parallel multi-LoRA)

================================================================================
CONFIGURATION FILES (/configs/)
================================================================================
All experiment configurations in YAML format.
Every config MUST have 'output_dir' field.

â”œâ”€â”€ configs/
â”‚   â””â”€â”€ experiments/
â”‚       â”œâ”€â”€ es_conciseness.yaml    Conciseness task config (2 train, 8 test)
â”‚       â””â”€â”€ es_countdown.yaml      Countdown task config (1600 train, 400 test)
â”‚                                  - Uses samples_per_generation: 128
â”‚                                  - Uses test_samples: 100

================================================================================
BASH SCRIPTS (/scripts/)
================================================================================
Minimal bash wrappers for running experiments.
Pattern: uv run python src/scripts/<script>.py configs/<config>.yaml "$@"

â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ experiments/
â”‚       â”œâ”€â”€ train_es_conciseness.sh    Run conciseness ES training
â”‚       â””â”€â”€ train_es_countdown.sh      Run countdown ES training

================================================================================
DATA OUTPUTS (/data/)
================================================================================
All experiment outputs (gitignored).
Auto-created from configs' output_dir field.

â”œâ”€â”€ data/                  Experiment outputs (gitignored)
â”‚   â”œâ”€â”€ tasks/             Task-specific datasets
â”‚   â”‚   â”œâ”€â”€ conciseness/
â”‚   â”‚   â”‚   â”œâ”€â”€ conciseness_train.json (2 examples)
â”‚   â”‚   â”‚   â””â”€â”€ conciseness_test.json  (8 examples)
â”‚   â”‚   â””â”€â”€ countdown/
â”‚   â”‚       â””â”€â”€ countdown.json         (2000 examples, split 80/20 at runtime)
â”‚   â”‚
â”‚   â””â”€â”€ <experiment_outputs>/  Auto-created from config's output_dir
â”‚       â”œâ”€â”€ figures/           Plots (evolution.png, time_breakdown.png)
â”‚       â”œâ”€â”€ results/           JSON results (evolution_history.json, time_breakdown.json)
â”‚       â”œâ”€â”€ logs/              Training logs
â”‚       â”œâ”€â”€ loras/             Generated LoRA adapters
â”‚       â””â”€â”€ config.yaml        Copy of config used

================================================================================
DOCUMENTATION (/docs/)
================================================================================
Research context, development logs, and technical documentation.

â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ start.txt                 Quick start guide (points to key docs)
â”‚   â”œâ”€â”€ closing_tasks.md          End-of-day checklist
â”‚   â”œâ”€â”€ structure.txt             This file - repo structure overview
â”‚   â”‚
â”‚   â”œâ”€â”€ CLAUDE.md -> ../CLAUDE.md Symlink to root CLAUDE.md
â”‚   â”‚
â”‚   â”œâ”€â”€ repo_usage.md             Comprehensive development guide (213 lines)
â”‚   â”‚                             - Fail-fast philosophy
â”‚   â”‚                             - Implementation vs orchestration
â”‚   â”‚                             - Config validation patterns
â”‚   â”‚                             - Standard script templates
â”‚   â”‚
â”‚   â”œâ”€â”€ research_context.md       Main research document (237 lines)
â”‚   â”‚                             - Project goals & methodology
â”‚   â”‚                             - ES algorithm details
â”‚   â”‚                             - Current progress & milestones
â”‚   â”‚                             - Open questions & hypotheses
â”‚   â”‚
â”‚   â””â”€â”€ logs/                     Daily development logs
â”‚       â””â”€â”€ 2025-10-21/
â”‚           â”œâ”€â”€ 1214_infrastructure_setup_and_testing.md
â”‚           â”œâ”€â”€ 1452_sglang_lora_fix.md
â”‚           â”œâ”€â”€ 1748_es_conciseness_task_complete_setup.md
â”‚           â”œâ”€â”€ 1932_es_formalization_and_task_refactoring.md
â”‚           â””â”€â”€ 2021_lora_init_debugging_and_countdown_setup.md

================================================================================
EXTERNAL RESOURCES (/resources/)
================================================================================
External code, papers, datasets (gitignored).

â”œâ”€â”€ resources/
â”‚   â””â”€â”€ es-fine-tuning-paper/     Git submodule - original paper's code
â”‚       â”œâ”€â”€ README.md
â”‚       â”œâ”€â”€ LICENSE.txt
â”‚       â”œâ”€â”€ requirement.txt
â”‚       â”œâ”€â”€ es_fine-tuning_conciseness.py
â”‚       â”œâ”€â”€ es_fine-tuning_conciseness_iid.py
â”‚       â”‚
â”‚       â””â”€â”€ countdown/            Countdown task implementation
â”‚           â”œâ”€â”€ countdown_task.py Reward function (imported by tests)
â”‚           â”œâ”€â”€ es_fine-tuning_countdown.py
â”‚           â”œâ”€â”€ es_fine-tuning_countdown_iid.py
â”‚           â””â”€â”€ data/
â”‚               â””â”€â”€ countdown.json    Dataset (5+ examples)

================================================================================
SCRATCH SPACE (/scratch/)
================================================================================
Temporary testing & exploration code (gitignored except .gitkeep).
NOT production code - for experimentation only.

â”œâ”€â”€ scratch/
â”‚   â”œâ”€â”€ .gitkeep
â”‚   â”‚
â”‚   â”œâ”€â”€ countdown_test/           Countdown task testing
â”‚   â”‚   â”œâ”€â”€ test_countdown.py     (183 lines) Base model inference test
â”‚   â”‚   â”œâ”€â”€ run_test.sh           Executable bash script
â”‚   â”‚   â””â”€â”€ outputs/              Test results directory
â”‚   â”‚
â”‚   â””â”€â”€ sglang_lora_test/         SGLang + LoRA integration testing
â”‚       â”œâ”€â”€ init_loras.py         (165 lines) Create 5 LoRA adapters
â”‚       â”œâ”€â”€ es_test_run.py        (389 lines) ES loop skeleton
â”‚       â”œâ”€â”€ run_init.sh           Run LoRA initialization
â”‚       â”œâ”€â”€ run_es_test.sh        Run ES test loop
â”‚       â”œâ”€â”€ start_sglang_server.sh    Start SGLang with multi-LoRA
â”‚       â”‚
â”‚       â”œâ”€â”€ lora_adapters/        Generated LoRA adapters
â”‚       â”‚   â”œâ”€â”€ lora_0/           Adapter 0 (rank=1, alpha=2)
â”‚       â”‚   â”‚   â”œâ”€â”€ adapter_config.json
â”‚       â”‚   â”‚   â””â”€â”€ adapter_model.safetensors
â”‚       â”‚   â”œâ”€â”€ lora_1/ ... lora_4/   Adapters 1-4 (same config)
â”‚       â”‚   â”œâ”€â”€ tokenizer/        Shared tokenizer
â”‚       â”‚   â””â”€â”€ metadata.json     Configuration tracking
â”‚       â”‚
â”‚       â””â”€â”€ es_test_output/       [Created at runtime]
â”‚                                  Evolution history & checkpoints

================================================================================
KEY DEPENDENCIES (from pyproject.toml)
================================================================================

CORE ML FRAMEWORKS
------------------
- torch                   PyTorch deep learning framework
- transformers            HuggingFace transformers (LLM inference)
- peft                    Parameter-Efficient Fine-Tuning (LoRA)
- accelerate              Distributed training utilities

SERVING & API
-------------
- fastapi                 Web service framework
- requests                HTTP client (for SGLang API)
- sglang                  [External] Multi-LoRA serving (not in pyproject)

EXPERIMENT TRACKING
-------------------
- wandb                   Weights & Biases experiment tracking

DATA SCIENCE
------------
- numpy                   Numerical computing
- scipy                   Scientific computing
- pandas                  Data manipulation
- scikit-learn            Machine learning utilities

PIPELINE ORCHESTRATION
----------------------
- dspy-ai                 Pipeline orchestration framework

DEVELOPMENT TOOLS
-----------------
- ipykernel               Jupyter kernel
- pytest-cov              Test coverage
- black                   Code formatting
- ruff                    Linting

================================================================================
CURRENT PROJECT STATUS
================================================================================

COMPLETED COMPONENTS
--------------------
âœ… Project structure (following CLAUDE.md)
âœ… Environment setup (uv, dependencies, .env)
âœ… Documentation (research context, development guidelines)
âœ… Task system (ConcisenessTask, CountdownTask with reward functions)
âœ… Production ES framework (src/scripts/es_train.py)
âœ… Experiment configs (conciseness, countdown)
âœ… Bash runners (train_es_conciseness.sh, train_es_countdown.sh)
âœ… SGLang + LoRA integration (dynamic load/unload working)
âœ… LoRA initialization system (adaptive per-layer + configurable scales)
âœ… Training subset cycling (for large datasets)
âœ… Time tracking and visualization

IN PROGRESS
-----------
ğŸ”„ Baseline experiments (ready to run, not yet executed)
ğŸ”„ Hyperparameter tuning (init_scale, perturb_scale values)

NOT YET STARTED
---------------
âŒ Experiment tracking (W&B integration)
âŒ Unit tests
âŒ Multi-GPU support
âŒ Checkpoint recovery

================================================================================
RESEARCH CONFIGURATION
================================================================================

MODEL
-----
Base: Qwen/Qwen2.5-7B-Instruct
LoRA config:
  - Rank: 1
  - Alpha: 2
  - Target layers: 0-10 (11 layers)
  - Target modules: q/k/v/o_proj (attn) + gate/up/down_proj (MLP)
  - Total: 77 target modules
  - Trainable params: ~0.01% of full model

ES ALGORITHM (Natural ES / Simplified NES)
------------------------------------------
Population size: 30
Learning rate (Î±): 0.01
Init scale: 4.0 (multiplier on per-layer base noise for Gen0)
Perturb scale: 1.0 (multiplier on per-layer base noise for Gen1+)
Base noise per layer: 10% of mean weight magnitude (adaptive)
Generations: 30 (conciseness) / 1000 (countdown)
Evaluation: Greedy decoding (temperature=0.0, deterministic)
Advantage: Z-score normalization (reward - mean) / std
Training: Cycles through subsets (countdown) or full dataset (conciseness)

TASKS
-----
Conciseness:
  - Dataset: data/tasks/conciseness/ (2 train, 8 test)
  - Reward: Word count reduction metric
  - Mode: Full dataset every generation

Countdown (mathematical reasoning):
  - Dataset: data/tasks/countdown/countdown.json (2000 total, split 80/20)
  - Reward: 0.1 Ã— format_reward + answer_reward
  - Mode: Cycles 128 samples/gen, 100 test samples (12.5x speedup)

================================================================================
KEY PATHS (Absolute)
================================================================================

Project Root:
/n/holylfs06/LABS/finkbeiner_lab/Users/cfpark00/datadir/evolm

Additional Working Directory:
/n/home12/cfpark00/datadir/evolm

Key Files:
- Countdown data: resources/es-fine-tuning-paper/countdown/data/countdown.json
- Countdown reward: resources/es-fine-tuning-paper/countdown/countdown_task.py
- Test LoRAs: scratch/sglang_lora_test/lora_adapters/
- Utils: src/utils.py

================================================================================
DEVELOPMENT WORKFLOW
================================================================================

1. CREATE EXPERIMENT CONFIG
   â””â”€ configs/experiments/<experiment_name>.yaml
      Must include: output_dir field

2. IMPLEMENT LOGIC
   â”œâ”€ Implementation (HOW): src/<module>.py
   â””â”€ Orchestration (WHAT/WHEN): src/scripts/<script>.py

3. CREATE BASH RUNNER
   â””â”€ scripts/<category>/<script>.sh
      Pattern: uv run python src/scripts/<script>.py configs/<config>.yaml "$@"

4. RUN EXPERIMENT
   â””â”€ bash scripts/<category>/<script>.sh [--overwrite] [--debug]

5. ANALYZE RESULTS
   â””â”€ data/<experiment_name>/
      â”œâ”€ figures/
      â”œâ”€ results/
      â””â”€ logs/

6. END OF DAY
   â”œâ”€ Write log: docs/logs/<YYYY-MM-DD>/<HHMM_topic>.md
   â”œâ”€ Update docs/structure.txt (if structure changed)
   â””â”€ Update docs/research_context.md (current status)

================================================================================
CODING CONVENTIONS
================================================================================

FAIL FAST PHILOSOPHY
--------------------
âœ… Crash immediately on missing configs
âœ… Validate all required fields upfront
âœ… Exit on critical errors (don't silently continue)
âŒ NO silent fallbacks or default values for critical params
âŒ NO hidden implicit behavior

IMPORTS
-------
from src.module import function         # âœ… Good
from src.utils import init_directory    # âœ… Good
from module import function             # âŒ Bad (relative)

CONFIG VALIDATION
-----------------
# Always check output_dir FIRST
if 'output_dir' not in config:
    raise ValueError("FATAL: 'output_dir' required in config")

# Validate other required fields
for field in required_fields:
    if field not in config:
        raise ValueError(f"FATAL: '{field}' required")

OUTPUT MANAGEMENT
-----------------
from src.utils import init_directory

output_dir = init_directory(config['output_dir'], overwrite=args.overwrite)
(output_dir / 'figures').mkdir(parents=True, exist_ok=True)
(output_dir / 'results').mkdir(parents=True, exist_ok=True)
(output_dir / 'logs').mkdir(parents=True, exist_ok=True)

# Copy config to output_dir for reproducibility
shutil.copy(config_path, output_dir / 'config.yaml')

================================================================================
GIT CONVENTIONS
================================================================================

GITIGNORED
----------
- /data/              All experiment outputs
- /scratch/           All testing code (except .gitkeep)
- /resources/         External resources
- /.venv/             Virtual environment
- /.env               Environment variables (use .env.example)

TRACKED
-------
- /src/               All production code
- /configs/           All configuration files
- /scripts/           All bash runners
- /docs/              All documentation
- uv.lock             Locked dependencies
- CLAUDE.md           Development guidelines

COMMIT MESSAGES
---------------
- Descriptive and concise
- Focus on WHY, not WHAT (code shows what)
- Use imperative mood ("Add feature" not "Added feature")

================================================================================
NOTES
================================================================================

1. This is a RESEARCH repository - prioritize clarity and reproducibility
   over performance optimization.

2. All code in /scratch/ is TEMPORARY - move to /src/ when production-ready.

3. NEVER create file variations like _v2, _final, _updated. Use git.

4. When in doubt, FAIL LOUDLY. Silent failures waste compute time.

5. Configs are the source of truth - everything else is code.

6. Documentation is not optional - it's part of research.

================================================================================
END OF STRUCTURE DOCUMENTATION
================================================================================

UPDATE LOG
----------
2025-10-21 20:21: Major production updates
  - Added production ES training (src/scripts/es_train.py)
  - Added task classes (src/tasks/conciseness.py, countdown.py)
  - Added experiment configs (configs/experiments/)
  - Added bash runners (scripts/experiments/)
  - Implemented LoRA init/perturb scale parameterization
  - Added training subset cycling for large datasets
  - Added time tracking and visualization
  - Ready for baseline experiments
