EVOLM - Evolution Strategies for LLM Fine-tuning
Repository Structure
Last Updated: 2025-10-21

================================================================================
ROOT DIRECTORY
================================================================================
/n/holylfs06/LABS/finkbeiner_lab/Users/cfpark00/datadir/evolm/

CONFIGURATION FILES
-------------------
├── CLAUDE.md              Project development guidelines (mandatory)
├── README.md              Basic project information
├── pyproject.toml         UV package configuration & dependencies
├── uv.lock                Locked dependencies for reproducibility
├── .env                   Environment variables (DATA_DIR=./data/)
├── .env.example           Template for environment setup
├── .gitignore             Git ignore rules (data/, .venv/, scratch/)

PYTHON VIRTUAL ENVIRONMENT
--------------------------
├── .venv/                 UV-managed Python virtual environment
                           (Use: source .venv/bin/activate OR uv run)

================================================================================
SOURCE CODE (/src/)
================================================================================
All production Python code lives here.

├── src/
│   ├── __init__.py
│   ├── utils.py           Safe directory initialization (init_directory)
│   │                      - Validates output paths against DATA_DIR
│   │                      - Prevents accidental overwrites outside data/
│   │                      - Used by all orchestration scripts
│   │
│   ├── tasks/             Task-specific logic (reward functions, data loading)
│   │   ├── __init__.py    Task registry (get_task function)
│   │   ├── conciseness.py ConcisenessTask class
│   │   └── countdown.py   CountdownTask class
│   │
│   └── scripts/           Orchestration scripts (entry points)
│       └── es_train.py    Main ES training loop (parallel multi-LoRA)

================================================================================
CONFIGURATION FILES (/configs/)
================================================================================
All experiment configurations in YAML format.
Every config MUST have 'output_dir' field.

├── configs/
│   └── experiments/
│       ├── es_conciseness.yaml    Conciseness task config (2 train, 8 test)
│       └── es_countdown.yaml      Countdown task config (1600 train, 400 test)
│                                  - Uses samples_per_generation: 128
│                                  - Uses test_samples: 100

================================================================================
BASH SCRIPTS (/scripts/)
================================================================================
Minimal bash wrappers for running experiments.
Pattern: uv run python src/scripts/<script>.py configs/<config>.yaml "$@"

├── scripts/
│   └── experiments/
│       ├── train_es_conciseness.sh    Run conciseness ES training
│       └── train_es_countdown.sh      Run countdown ES training

================================================================================
DATA OUTPUTS (/data/)
================================================================================
All experiment outputs (gitignored).
Auto-created from configs' output_dir field.

├── data/                  Experiment outputs (gitignored)
│   ├── tasks/             Task-specific datasets
│   │   ├── conciseness/
│   │   │   ├── conciseness_train.json (2 examples)
│   │   │   └── conciseness_test.json  (8 examples)
│   │   └── countdown/
│   │       └── countdown.json         (2000 examples, split 80/20 at runtime)
│   │
│   └── <experiment_outputs>/  Auto-created from config's output_dir
│       ├── figures/           Plots (evolution.png, time_breakdown.png)
│       ├── results/           JSON results (evolution_history.json, time_breakdown.json)
│       ├── logs/              Training logs
│       ├── loras/             Generated LoRA adapters
│       └── config.yaml        Copy of config used

================================================================================
DOCUMENTATION (/docs/)
================================================================================
Research context, development logs, and technical documentation.

├── docs/
│   ├── start.txt                 Quick start guide (points to key docs)
│   ├── closing_tasks.md          End-of-day checklist
│   ├── structure.txt             This file - repo structure overview
│   │
│   ├── CLAUDE.md -> ../CLAUDE.md Symlink to root CLAUDE.md
│   │
│   ├── repo_usage.md             Comprehensive development guide (213 lines)
│   │                             - Fail-fast philosophy
│   │                             - Implementation vs orchestration
│   │                             - Config validation patterns
│   │                             - Standard script templates
│   │
│   ├── research_context.md       Main research document (237 lines)
│   │                             - Project goals & methodology
│   │                             - ES algorithm details
│   │                             - Current progress & milestones
│   │                             - Open questions & hypotheses
│   │
│   └── logs/                     Daily development logs
│       └── 2025-10-21/
│           ├── 1214_infrastructure_setup_and_testing.md
│           ├── 1452_sglang_lora_fix.md
│           ├── 1748_es_conciseness_task_complete_setup.md
│           ├── 1932_es_formalization_and_task_refactoring.md
│           └── 2021_lora_init_debugging_and_countdown_setup.md

================================================================================
EXTERNAL RESOURCES (/resources/)
================================================================================
External code, papers, datasets (gitignored).

├── resources/
│   └── es-fine-tuning-paper/     Git submodule - original paper's code
│       ├── README.md
│       ├── LICENSE.txt
│       ├── requirement.txt
│       ├── es_fine-tuning_conciseness.py
│       ├── es_fine-tuning_conciseness_iid.py
│       │
│       └── countdown/            Countdown task implementation
│           ├── countdown_task.py Reward function (imported by tests)
│           ├── es_fine-tuning_countdown.py
│           ├── es_fine-tuning_countdown_iid.py
│           └── data/
│               └── countdown.json    Dataset (5+ examples)

================================================================================
SCRATCH SPACE (/scratch/)
================================================================================
Temporary testing & exploration code (gitignored except .gitkeep).
NOT production code - for experimentation only.

├── scratch/
│   ├── .gitkeep
│   │
│   ├── countdown_test/           Countdown task testing
│   │   ├── test_countdown.py     (183 lines) Base model inference test
│   │   ├── run_test.sh           Executable bash script
│   │   └── outputs/              Test results directory
│   │
│   └── sglang_lora_test/         SGLang + LoRA integration testing
│       ├── init_loras.py         (165 lines) Create 5 LoRA adapters
│       ├── es_test_run.py        (389 lines) ES loop skeleton
│       ├── run_init.sh           Run LoRA initialization
│       ├── run_es_test.sh        Run ES test loop
│       ├── start_sglang_server.sh    Start SGLang with multi-LoRA
│       │
│       ├── lora_adapters/        Generated LoRA adapters
│       │   ├── lora_0/           Adapter 0 (rank=1, alpha=2)
│       │   │   ├── adapter_config.json
│       │   │   └── adapter_model.safetensors
│       │   ├── lora_1/ ... lora_4/   Adapters 1-4 (same config)
│       │   ├── tokenizer/        Shared tokenizer
│       │   └── metadata.json     Configuration tracking
│       │
│       └── es_test_output/       [Created at runtime]
│                                  Evolution history & checkpoints

================================================================================
KEY DEPENDENCIES (from pyproject.toml)
================================================================================

CORE ML FRAMEWORKS
------------------
- torch                   PyTorch deep learning framework
- transformers            HuggingFace transformers (LLM inference)
- peft                    Parameter-Efficient Fine-Tuning (LoRA)
- accelerate              Distributed training utilities

SERVING & API
-------------
- fastapi                 Web service framework
- requests                HTTP client (for SGLang API)
- sglang                  [External] Multi-LoRA serving (not in pyproject)

EXPERIMENT TRACKING
-------------------
- wandb                   Weights & Biases experiment tracking

DATA SCIENCE
------------
- numpy                   Numerical computing
- scipy                   Scientific computing
- pandas                  Data manipulation
- scikit-learn            Machine learning utilities

PIPELINE ORCHESTRATION
----------------------
- dspy-ai                 Pipeline orchestration framework

DEVELOPMENT TOOLS
-----------------
- ipykernel               Jupyter kernel
- pytest-cov              Test coverage
- black                   Code formatting
- ruff                    Linting

================================================================================
CURRENT PROJECT STATUS
================================================================================

COMPLETED COMPONENTS
--------------------
✅ Project structure (following CLAUDE.md)
✅ Environment setup (uv, dependencies, .env)
✅ Documentation (research context, development guidelines)
✅ Task system (ConcisenessTask, CountdownTask with reward functions)
✅ Production ES framework (src/scripts/es_train.py)
✅ Experiment configs (conciseness, countdown)
✅ Bash runners (train_es_conciseness.sh, train_es_countdown.sh)
✅ SGLang + LoRA integration (dynamic load/unload working)
✅ LoRA initialization system (adaptive per-layer + configurable scales)
✅ Training subset cycling (for large datasets)
✅ Time tracking and visualization

IN PROGRESS
-----------
🔄 Baseline experiments (ready to run, not yet executed)
🔄 Hyperparameter tuning (init_scale, perturb_scale values)

NOT YET STARTED
---------------
❌ Experiment tracking (W&B integration)
❌ Unit tests
❌ Multi-GPU support
❌ Checkpoint recovery

================================================================================
RESEARCH CONFIGURATION
================================================================================

MODEL
-----
Base: Qwen/Qwen2.5-7B-Instruct
LoRA config:
  - Rank: 1
  - Alpha: 2
  - Target layers: 0-10 (11 layers)
  - Target modules: q/k/v/o_proj (attn) + gate/up/down_proj (MLP)
  - Total: 77 target modules
  - Trainable params: ~0.01% of full model

ES ALGORITHM (Natural ES / Simplified NES)
------------------------------------------
Population size: 30
Learning rate (α): 0.01
Init scale: 4.0 (multiplier on per-layer base noise for Gen0)
Perturb scale: 1.0 (multiplier on per-layer base noise for Gen1+)
Base noise per layer: 10% of mean weight magnitude (adaptive)
Generations: 30 (conciseness) / 1000 (countdown)
Evaluation: Greedy decoding (temperature=0.0, deterministic)
Advantage: Z-score normalization (reward - mean) / std
Training: Cycles through subsets (countdown) or full dataset (conciseness)

TASKS
-----
Conciseness:
  - Dataset: data/tasks/conciseness/ (2 train, 8 test)
  - Reward: Word count reduction metric
  - Mode: Full dataset every generation

Countdown (mathematical reasoning):
  - Dataset: data/tasks/countdown/countdown.json (2000 total, split 80/20)
  - Reward: 0.1 × format_reward + answer_reward
  - Mode: Cycles 128 samples/gen, 100 test samples (12.5x speedup)

================================================================================
KEY PATHS (Absolute)
================================================================================

Project Root:
/n/holylfs06/LABS/finkbeiner_lab/Users/cfpark00/datadir/evolm

Additional Working Directory:
/n/home12/cfpark00/datadir/evolm

Key Files:
- Countdown data: resources/es-fine-tuning-paper/countdown/data/countdown.json
- Countdown reward: resources/es-fine-tuning-paper/countdown/countdown_task.py
- Test LoRAs: scratch/sglang_lora_test/lora_adapters/
- Utils: src/utils.py

================================================================================
DEVELOPMENT WORKFLOW
================================================================================

1. CREATE EXPERIMENT CONFIG
   └─ configs/experiments/<experiment_name>.yaml
      Must include: output_dir field

2. IMPLEMENT LOGIC
   ├─ Implementation (HOW): src/<module>.py
   └─ Orchestration (WHAT/WHEN): src/scripts/<script>.py

3. CREATE BASH RUNNER
   └─ scripts/<category>/<script>.sh
      Pattern: uv run python src/scripts/<script>.py configs/<config>.yaml "$@"

4. RUN EXPERIMENT
   └─ bash scripts/<category>/<script>.sh [--overwrite] [--debug]

5. ANALYZE RESULTS
   └─ data/<experiment_name>/
      ├─ figures/
      ├─ results/
      └─ logs/

6. END OF DAY
   ├─ Write log: docs/logs/<YYYY-MM-DD>/<HHMM_topic>.md
   ├─ Update docs/structure.txt (if structure changed)
   └─ Update docs/research_context.md (current status)

================================================================================
CODING CONVENTIONS
================================================================================

FAIL FAST PHILOSOPHY
--------------------
✅ Crash immediately on missing configs
✅ Validate all required fields upfront
✅ Exit on critical errors (don't silently continue)
❌ NO silent fallbacks or default values for critical params
❌ NO hidden implicit behavior

IMPORTS
-------
from src.module import function         # ✅ Good
from src.utils import init_directory    # ✅ Good
from module import function             # ❌ Bad (relative)

CONFIG VALIDATION
-----------------
# Always check output_dir FIRST
if 'output_dir' not in config:
    raise ValueError("FATAL: 'output_dir' required in config")

# Validate other required fields
for field in required_fields:
    if field not in config:
        raise ValueError(f"FATAL: '{field}' required")

OUTPUT MANAGEMENT
-----------------
from src.utils import init_directory

output_dir = init_directory(config['output_dir'], overwrite=args.overwrite)
(output_dir / 'figures').mkdir(parents=True, exist_ok=True)
(output_dir / 'results').mkdir(parents=True, exist_ok=True)
(output_dir / 'logs').mkdir(parents=True, exist_ok=True)

# Copy config to output_dir for reproducibility
shutil.copy(config_path, output_dir / 'config.yaml')

================================================================================
GIT CONVENTIONS
================================================================================

GITIGNORED
----------
- /data/              All experiment outputs
- /scratch/           All testing code (except .gitkeep)
- /resources/         External resources
- /.venv/             Virtual environment
- /.env               Environment variables (use .env.example)

TRACKED
-------
- /src/               All production code
- /configs/           All configuration files
- /scripts/           All bash runners
- /docs/              All documentation
- uv.lock             Locked dependencies
- CLAUDE.md           Development guidelines

COMMIT MESSAGES
---------------
- Descriptive and concise
- Focus on WHY, not WHAT (code shows what)
- Use imperative mood ("Add feature" not "Added feature")

================================================================================
NOTES
================================================================================

1. This is a RESEARCH repository - prioritize clarity and reproducibility
   over performance optimization.

2. All code in /scratch/ is TEMPORARY - move to /src/ when production-ready.

3. NEVER create file variations like _v2, _final, _updated. Use git.

4. When in doubt, FAIL LOUDLY. Silent failures waste compute time.

5. Configs are the source of truth - everything else is code.

6. Documentation is not optional - it's part of research.

================================================================================
END OF STRUCTURE DOCUMENTATION
================================================================================

UPDATE LOG
----------
2025-10-21 20:21: Major production updates
  - Added production ES training (src/scripts/es_train.py)
  - Added task classes (src/tasks/conciseness.py, countdown.py)
  - Added experiment configs (configs/experiments/)
  - Added bash runners (scripts/experiments/)
  - Implemented LoRA init/perturb scale parameterization
  - Added training subset cycling for large datasets
  - Added time tracking and visualization
  - Ready for baseline experiments
